# Introduction

In this project, we propose MTBoost, a reinforcement-learning algorithm to improve robustness for DNN based classifiers.
In our project, the robustness of a DNN is evaluated by metamorphic relations (MRs), necessary properties of a DNN classifier.
Normally, an MR defines such that F(x) = F(x'), where F is the DNN model, x is then input, and x' = T(x), a transformation of x.
For example, x is an image of an object, and x' is generated by rotating x by 30 degrees.
Since rotation cannot change the label of an object, then we should have F(x) = F(x').
For a DNN model, since its output are stochastic, for some inputs, we have F(x) = F(x'), while for others, we have F(x) \ne F(x').
Therefore, we use the probability of Pr{F(x) = F(x')}, or Pr{F(x') = y}, two types of metrics to measure to what extent a DNN satisfying an MR.
Recall that, a DNN classifier should be accurate, which is defined as the probability of the DNN can correctly label an input, which is Pr{F(x) = y}.
Therefore, the robustness evaluated by MRs are different from accuracy, and can be treated as the second metrics to assess a DNN classifier.

In our project, our goal is to improve the robustness of DNN classifier with a given set of MRs without damaging its accuracy.
Therefore, we have three goals:
(1) maximize Pr{F(x) = y}
(2) maximize Pr{F(x') = y}
(3) maximize Pr{f(x) = f(x')}
Here, F(x) stands for the final output of DNN while f(x) stands for the raw output of DNN without finding the maximized item (the label).

It should be noted that, (1) and (2) may not necessarily leading to (3).
We design Goals (1) and (2) to let the DNN to learn the knowledge from human.
We design Goal (3) to let the DNN to be stable or to be more "precise", since x and x' are related and their calculation process should be similar.

To achieve Goals (1) (2) and (3), first, we designed as loss function with two regularization items:
Loss = loss1 + \alpha loss2 + \beta loss3, where loss2 and loss3 are two regularization items.
Suppose loss1 is cross_entropy(F(x), y), the cross_entropy between F(x) and y, then we have
- loss1 = cross_entropy(F(x), y)
- loss2 = cross_entropy(F(x'), y)
- loss3 = kl_divergence(f(x), f(x')), where kl_divergence is Kullback–Leibler divergence.
Here, \alpha and \beta are used to adjust the importance among three goals.
Second, achieving Goals (1) (2), and (3) transfers to minimizing Loss.

However, this introduce a second questions: how to select x'? Recall that x' = T(x).
Let's break this problem down.
(1) Usually, there is parameter (or multiple parameters) in T.
For example, T(x;30) can be mean rotating x with 30 degrees.
(2) There are multiple MRs, that is we have T1, T2, ...., Tn, and each has parameters.
One solution is to compound all Ts together, such that x' = T1(T2, ...，Tn(x;pn)...;p2);p1)
Therefore, the problem of choosing x' transfers to the problem of determining the values of p1, p2, ..., pn.


To determine [p1, p2, ..., pn], our method is to maximize loss2, that is:
 [p1, p2, ..., pn] = argmax(cross_entropy(F(x'), y)), where x' = T1(T2, ...，Tn(x;pn)...;p2);p1)
Normally, there are many traditional or heuristic searching methods to solve the above question.
Here, we found they are not effective or hard to be used.

In this project, we propose to use reinforcement learning algorithm to deal with this problem.
We transfer the maximizing optimization problem into a reinforcement-learning problem by introdcing the actions, the rewards, and the observations.
- the actions corresponds to [p1, p2, ..., pn]
- the rewards is a function of cross_entropy(F(x'), y)
- the observation is cross_entropy(F(x'), y)
With this setting, we tested four types of reinforcement learning algorithms, namely SAC, DDPG, PPO, and TD3.
We found SAC achieves the best performances in terms of training results and training durations.

In this project, we provides all the details about our experiments.

# The structure of our repository

## exp_platform.py (the main function or the entrance of our experiments)

The purpose of this file is to configure, run and logging.
- there are 7 functions corresponding to RQs 1.1, 1.2, 1.3, 2.1, 2.2, 3.1 and 3.2.
- there is a TrainStrategyFactory. There are 11 normal training strategies and 4 ablation training implemented in our projects.
-- There are 11 normal training strategies: Standard, Random, W10, W10KL, SAC, DDPG, TD3, PPO, SENSEI, AUTOMIX, and AUTOAUGMENT.
-- There are 4 ablation strategies: SAC-ll, DDPG-ll, TD3-ll, and PPO-ll
-- There are two hyper-parameter effect analysis experiments
--- changing the test size
--- changing alpha and beta
- there is a get_dataset function, which is used to prepare the dataset. There are four datasets used: SVHN, CIFAR10, GTSRB, and FashionMNIST
- there is a get_model function, which is used to prepare the models for each dataset. For each dataset, we have two models.

For each configuration (or treatment), we collect 5 samples. During each configuration, the training epoch is set to be 100.

## Datasets.py
The purpose of include all the configurations of the four datasets into a file and wrap them into classes for better usage.
In this project, we designed a abstract dataset class and derived five types of datasets.

# TrainingStrategies.py
In this file, all the training strategies are implemented.
There is a abstract class, and other strategies are derived from it.

# models
In this directory, all the DNN models are implemented.

# RLStrategies
In this directory, we implement the four types of reinforcement learning strategies.

# data
In this directory, all the training data downloaded from the internet are stored.

# SavedCheckpoints
This is directory used to save all the training pt files. We did not upload to Github because it is too large.

# Logs
This is directory used to save all the effectiveness and the training durations information.
For each dataset, we prepared one file to save.

# DataRecordsAndAnalysis
We gathered all the results (saved in "Logs") into a "log.csv" file and wrote a python script (.ipynb) to analyze it.